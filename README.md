
---

# ğŸ” GraphLIME: Interpretable Explanations for Graph Neural Networks


## ğŸ§  Project Overview

This project implements **GraphLIME**, a **model-agnostic explanation framework** for Graph Neural Networks (GNNs) that generates **locally faithful** and **interpretable explanations** of node-level predictions in graph-structured data.

GraphLIME addresses the **black-box nature** of GNNs by selecting the most relevant node features within an N-hop neighborhood using **HSIC Lasso**, a kernel-based nonlinear feature selection technique. The method is benchmarked against other popular interpretable models such as **LIME**, **GNNExplainer**, and **Greedy selection**.

---

## ğŸ“‚ Repository Contents

* `GCN_and_graphlime.ipynb` â€” Jupyter Notebook demonstrating:

  * Training of GNN models (GraphSAGE, GAT)
  * Implementing GraphLIME and LIME
  * Explanation results and visualizations

* `patel_shrey_presentation.pptx` â€” Presentation summarizing the paper, implementation, experimental results, and key observations.

---

## ğŸ“ˆ Key Features

* âœ… **Model-Agnostic**: Works with any GNN (e.g., GAT, GraphSAGE)
* ğŸ” **N-hop Neighborhood Sampling**: Local exploration around the node of interest
* ğŸ” **HSIC Lasso**: Captures nonlinear dependencies to extract top-K informative features
* ğŸ”¬ **Sparse Explanations**: Focus on the most critical features for interpretability
* ğŸ“Š **Comparison with LIME**: Highlights effectiveness in filtering noisy features

---

## ğŸ“Š Datasets Used

* **Cora**: Citation network with binary feature vectors
* **Pubmed**: Publication network with TF-IDF-based continuous features

Both datasets are commonly used benchmarks in GNN research.

---

## ğŸ§ª Evaluation Metrics

* **Noise Distribution Analysis**: Measures how well each explainer avoids selecting irrelevant features
* **F1 Trustworthiness Score**: Validates explanations with respect to model fidelity
* **Classifier Selection Accuracy**: Whether explanations help in choosing better models

GraphLIME consistently **outperformed** baseline methods across all metrics.

---

## ğŸ“Œ Observations

| Method       | Explanation Type | Noise Filtering | Local Faithfulness | Nonlinear Capture |
| ------------ | ---------------- | --------------- | ------------------ | ----------------- |
| GraphLIME    | Nonlinear        | âœ… Excellent     | âœ… Strong           | âœ… Yes             |
| LIME         | Linear           | âŒ Poor          | âŒ Weak             | âŒ No              |
| GNNExplainer | Subgraph-based   | âš ï¸ Moderate     | âœ… Fair             | âš ï¸ Partial        |

more:

<img width="700" height="300" alt="image" src="https://github.com/user-attachments/assets/b020debc-2da5-4784-bd4f-7d04c04b95c8" />

---

## ğŸ“š Reference

ğŸ“„ [IEEE Paper: *GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks*](https://ieeexplore.ieee.org/document/9811416)
Authors: Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, Yi Chang
Affiliations: Jilin University, Kyoto University, RIKEN AIP

---

## ğŸ§­ Future Directions

* ğŸ”„ Extend GraphLIME to **subgraph-level** explanations
* ğŸ‘¥ Explain predictions for **groups of nodes**
* ğŸ“ˆ Improve **scalability** to large, dynamic graphs

---


